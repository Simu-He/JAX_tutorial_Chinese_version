{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-transmission",
   "metadata": {},
   "source": [
    "# JAX快速开始\n",
    "\n",
    "`JAX` 是CPU, GPU和TPU上的Numpy实现，具有出色的自动求导功能，可用于高性能机器学习研究。\n",
    "\n",
    "使用最新的 [Autograd](https://github.com/hips/autograd) 库，`JAX`可以自动求导原生的Python和Numpy代码。在Python的循环、条件、递归和闭包中也能够轻松使用，甚至可以求微分的微分的微分。它也支持反向模式和正向模式微分，并且以任意的顺序组合。\n",
    "\n",
    "`JAX`使用 [XLA](https://www.tensorflow.org/xla) 来在GPU或TPU加速器上编译和运行代码。默认情况下编译在后台进行，并且库调用会即时编译（JIT）和执行。`JAX`甚至可以仅用一条函数API来让你将自己写的Python函数即时编译成XLA优化核。您可以任意组合编译和自动微分，无需离开Python即可变大复杂的算法并且获得最佳的性能。\n",
    "\n",
    "首先我们先导入常用的JAX库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "checked-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-payday",
   "metadata": {},
   "source": [
    "## 矩阵乘法\n",
    "\n",
    "我们用以下的示例来生成随机数据。与Numpy的一个较大的不同点是，JAX和它生成随机数的方式不同。详细内容参考[Common Gotchas in JAX](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sporting-basics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10, ))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-payment",
   "metadata": {},
   "source": [
    "让我们现在开始，给两个大矩阵做乘法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-procurement",
   "metadata": {},
   "source": [
    "我们添加了 `block_until_ready`，因为默认情况下JAX采用异步执行（详见 异步调度）。\n",
    "\n",
    "JAX的NumPy函数也可以用于普通的NumPy数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-court",
   "metadata": {},
   "source": [
    "这样做会比较慢，因为每次都必须将数据传送到GPU。您可以使用 `device_put()`确保 `NDArray`由设备内存支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-reduction",
   "metadata": {},
   "source": [
    "`device_put()` 的输出仍然类似 `NDArray`，但仅需在打印、绘图、保存和分支等需要它们的值的时候才将值复制回CPU。 `device_put()`的行为等效于函数 `jit(lambda x: x)`，但速度更快。\n",
    "\n",
    "如果你有GPU或TPU，这些调用都会在加速设备上云子那个，并且有比CPU更快的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit np.dot(x, x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-characteristic",
   "metadata": {},
   "source": [
    "JAX不仅仅是一个由GPU支持的NumPy。他还带有一些程序转换，这些转换在编写数值代码的时候很有用。目前主要有三个：\n",
    "\n",
    "* `jit()`，用于加速你的代码\n",
    "* `grad()`，用于微分\n",
    "* `vmap()`，用于自动向量化或批处理\n",
    "\n",
    "接下来我们一一介绍，我们还将以有趣的方式来编写这些内容。\n",
    "\n",
    "## 使用 `jit()`来加速你的代码\n",
    "\n",
    "JAX可以在GPU（或CPU，如果您没有GPU的话，TPU支持即将到来！）上透明地运行。但是，在上面的实例中，JAX一次只将一个内核分配给GPU操作。如果有一系列操作，则可以使用 `@jit` 装饰器使用XLA一起编译多个操作。让我们尝试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000, ))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-teach",
   "metadata": {},
   "source": [
    "我们可以用 `@jit`加快速度，它将在首次调用 `selu` 的时候进行jit编译，然后将其缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-attachment",
   "metadata": {},
   "source": [
    "## 使用`grad()`来微分\n",
    "\n",
    "除了计算数值函数以外，我们还希望对其进行变换。一种变换是[自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)。在JAX中，就像 [Autograd](https://github.com/HIPS/autograd)，你可以使用`grad()`函数计算微分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-highland",
   "metadata": {},
   "source": [
    "让我们使用差分来验证我们的计算结果是正确的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_finite_differences(f, x):\n",
    "    eps = 1e-3\n",
    "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps) for v in jnp.eye(len(x))])\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-stewart",
   "metadata": {},
   "source": [
    "求微分就像调用 `grad()`一样容易。`grad()`和`jit()`可以任意组合。在上面的示例中，我们将`sum_logistic`设置为即时编译，然后取其微分。我们也可以更进一步："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-survival",
   "metadata": {},
   "source": [
    "对于更高级的自动微分，可以调用`jax.vjp()`用于反向模式的向量-雅克比积(vector-Jacobian products)和`jax.jvp()`用于前向模式的雅克比-向量积(Jacobian-vector products)。这两者也可以任意组合彼此，也可以与其他的JAX转换互相组合。这里提供了一种他们构成有效计算完整的Hessian矩阵函数的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-wireless",
   "metadata": {},
   "source": [
    "## 使用`vmap()`来自动向量化\n",
    "\n",
    "JAX在其API中还有另一种可能有用的转换:`vmap()`，向量化映射。它具有沿数组轴映射函数的类似语义，但不是将循环保留在外部，而是将循环推入函数的原始操作中以提高性能。当与`jit()`组合时，它的速度可以与手动添加批梯度一样快。这里我们将使用一个简单的示例，并使用 `vmap()`将矩阵-向量乘积提升为矩阵-矩阵乘积。尽管在这种特定情况下很容易手动完成此操作，但是这种技术可以用于更加复杂的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-utilization",
   "metadata": {},
   "source": [
    "给定诸如`apply_matrix()`之类的函数，我们可以在Python中循环执行批处理维度，但是这么做的性能很差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-department",
   "metadata": {},
   "source": [
    "我们知道如何手动批处理该操作。在这种情况下，`jnp.dot()`能够透明地处理额外的批处理维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "    return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-reservation",
   "metadata": {},
   "source": [
    "然而，假设没有批处理支持，我们的函数可能更加复杂。我们可以使用`vmap()`自动添加批处理支持："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "    return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vecctorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-valley",
   "metadata": {},
   "source": [
    "当然，`vmap()`可以和`jit()`，`grad()`和任何其他JAX变换任意组合。这只是JAX的冰山一角，我们很兴奋您将使用JAX！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
